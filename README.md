# RL-Temporal-Difference-Learning

Ce projet permet de determiner la valeur de lambda √† partir de deux m√©thodes: le Temporal difference learning et le Monte Carlo learning. Nous appuyons sur le Markov Decision Processus(MDP). 

# Description du probl√®me

Given an MDP and a particular time step $t$ of a task (continuing or
episodic), the $\lambda$-return, $G_t^\lambda$, $0\leq\lambda\leq 1$, is
a weighted combination of the $n$-step returns $G_{t:t+n}$, $n \geq 1$:

$${G_t^\lambda = \sum\limits_{n=1}^\infty(1-\lambda)\lambda^{n-1}G_{t:t+n}.}$$

While the $n$-step return $G_{t:t+n}$ can be viewed as the target of
an $n$-step TD update rule, the $\lambda$-return can be viewed as the
target of the update rule for the TD$(\lambda)$ prediction algorithm,
which you will become familiar with in project 1.

Consider the Markov reward process described by the following state
diagram and assume the agent is in state $0$ at time $t$ (also assume
the discount rate is $\gamma=1$). A Markov reward process can be thought of as
an MDP with only one action possible from each state (denoted as action $0$ in
the figure below).

![image](https://d1b10bmlvqabco.cloudfront.net/paste/jqmfo7d3watba/9e6fd83672f880704b8418728297fc077786c8907d87fec631601da9ff4c85ef/hw2.png)

# Mod√©lisation

Pour r√©soudre ce probl√®me, nous avons d√©termin√© :

- les diff√©rents √©tats (situation) d√©crits dans l'√©nonc√© du probl√®me: {1, 2,....., N}
- les actions qui peuvent √™tre r√©alis√©es depuis chaque √©tats
- les gains et les pertes potentiels associ√©s √† chaque action r√©alis√©e par le jouer 0 <= X < N

Le but √©tant de trouver la valeur de lambda qui permet d'obtenir les gains (rewards) et valeur donn√©es dans l'√©nonc√©, ceci √† partir des formules r√©quises 

ùëáùê∑(ùúÜ) return has the following form:

 $${G_t^\lambda = \sum\limits_{n=1}^\infty(1-\lambda)\lambda^{n-1}G_{t:t+n}.}$$

```BibTeX

  @misc{For you MDP ùëáùê∑(1) looks like this:

        ùê∫=p*(ùëü0+ùëü2+ùëü4+ùëü5+ùëü6)+(1-p)*(ùëü1+ùëü3+ùëü4+ùëü5+ùëü6) avec p = probabilit√© de transition

    ùëáùê∑(ùúÜ)  looks like this:

        ùê∫ùúÜ0=(1‚àíùúÜ)[ùúÜ^0*ùê∫0:1+ùúÜ^1*ùê∫0:2+ùúÜ^2*ùê∫0:3+ùúÜ^3*ùê∫0:4+ùúÜ^4*ùê∫0:5]

    enfin, il faudrait resoudre l'√©quation suivante pour obtenir la valuer de ùúÜ:
        (ùê∫0:1-G)+(ùê∫0:2-ùê∫0:1)*ùúÜ+(ùê∫0:3-ùê∫0:2)*ùúÜ^2 +(ùê∫0:4-ùê∫0:3)*ùúÜ^3+(ùê∫0:5-ùê∫0:4)*ùúÜ^4+(-ùê∫0:5)*ùúÜ^5 = 0
  }

```

```BibTeX
@misc{jmc2022RL-Temporal-Difference-Learning,
  author =       {Jean-Michel Cheumeni},
  title =        {RL},
  howpublished = {\url{git@github.com:cheumeni-commit/RL-Temporal-Difference-Learning.git}},
  year =         {2022}
}
```